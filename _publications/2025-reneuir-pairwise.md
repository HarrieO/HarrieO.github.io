---
title: "Harnessing Pairwise Ranking Prompting Through Sample-Efficient Ranking Distillation"
authors: "Junru Wu, Le Yan, Zhen Qin, Honglei Zhuang, Tianqi Liu, Zhe Dong, Xuanhui Wang, Harrie Oosterhuis"
collection: publications
permalink: /publication/2025-reneuir-pairwise
excerpt: "We propose to harness the effectiveness of PRP through pairwise distillation. Specifically, we distill a pointwise student ranker from pairwise teacher labels generated by PRP, resulting in an efficient student model that retains the performance of PRP with substantially lower computational costs."
date: 2025-07-17
venue: 'Proceedings of ReNeuIR at SIGIR 2025: The Fourth Workshop on Reaching Efficiency in Neural Information Retrieval (ReNeuIR ’25)'
paperurl: http://harrieo.github.io/files/2025-reneuir-pairwise.pdf
citation: "Wu, J., Yan, L., Qin, Z., Zhuang, H., Liu, T., Dong, Z., Wang, X., Oosterhuis, H., (2025, July). Harnessing Pairwise Ranking Prompting Through Sample-Efficient Ranking Distillation. In Proceedings of ReNeuIR at SIGIR 2025: The Fourth Workshop on Reaching Efficiency in Neural Information Retrieval (ReNeuIR ’25), Padua, Italy, 2025."
youtube: 
codeurl: 
othervideo:
tutorialwebsite: 
slides: 
googleslidesembed: 
---

While Pairwise Ranking Prompting (PRP) with Large Language Models (LLMs) is one of the most effective zero-shot document ranking methods, it has a quadratic computational complexity with respect to the number of documents to be ranked, as it requires an enumeration over all possible document pairs. Consequently, the outstanding ranking performance of PRP has remained unreachable for most real-world ranking applications.

In this work, we propose to harness the effectiveness of PRP through pairwise distillation. Specifically, we distill a pointwise student ranker from pairwise teacher labels generated by PRP, resulting in an efficient student model that retains the performance of PRP with substantially lower computational costs. Furthermore, we find that the distillation process can be made sample-efficient: with only 2% of pairs, we are able to obtain the same performance as using all pairs for teacher labels. Thus, our novel approach provides a solution to harness the ranking performance of PRP without incurring high computational costs during both distillation and serving.
