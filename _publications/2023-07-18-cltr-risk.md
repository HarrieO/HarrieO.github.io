---
title: "Safe Deployment for Counterfactual Learning to Rank with Exposure-Based Risk Minimization"
authors: "Shashank Gupta, Harrie Oosterhuis, and Maarten de Rijke"
collection: publications
permalink: /publication/2023-cltr-risk
excerpt: "We introduce a novel risk-aware CLTR method with theoretical guarantees for safe deployment. We apply a novel exposure-based concept of risk regularization to IPS estimation for LTR. Our risk regularization penalizes the mismatch between the ranking behavior of a learned model and a given safe model. Thereby, it ensures that learned ranking models stay close to a trusted model, when there is high uncertainty in IPS estimation, which greatly reduces the risks during deployment."
date: 2023-07-18
venue: 'Proceedings of the 46nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR â€™23)'
paperurl: http://harrieo.github.io/files/2023-sigir-crm-ultr.pdf
citation: "Gupta, S., Oosterhuis, H., & de Rijke, M. (2023, July). Safe deployment for counterfactual learning to rank with exposure-based risk minimization. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (pp. 249-258)."
youtube: 
codeurl: https://github.com/shashankg7/crm_ultr
othervideo:
tutorialwebsite: 
slides: 
googleslidesembed: 
---

Counterfactual learning to rank (CLTR) relies on exposure-based inverse propensity scoring (IPS), a LTR-specific adaptation of IPS to correct for position bias. While IPS can provide unbiased and consistent estimates, it often suffers from high variance. Especially when little click data is available, this variance can cause CLTR to learn sub-optimal ranking behavior. Consequently, existing CLTR methods bring significant risks with them, as naively deploying their models can result in very negative user experiences.

We introduce a novel risk-aware CLTR method with theoretical guarantees for safe deployment. We apply a novel exposure-based concept of risk regularization to IPS estimation for LTR. Our risk regularization penalizes the mismatch between the ranking behavior of a learned model and a given safe model. Thereby, it ensures that learned ranking models stay close to a trusted model, when there is high uncertainty in IPS estimation, which greatly reduces the risks during deployment. Our experimental results demonstrate the efficacy of our proposed method, which is effective at avoiding initial periods of bad performance when little date is available, while also maintaining high performance at convergence. For the CLTR field, our novel exposure-based risk minimization method enables practitioners to adopt CLTR methods in a safer manner that mitigates many of the risks attached to previous methods.
